# -*- coding: utf-8 -*-
"""llm_fine_tuning(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_9Wu4e4qYfVWYivynUda1HXKagbC-Zpw
"""

# Libraries for
!pip install -q --upgrade transformers datasets accelerate torch torchvision peft pillow

from transformers import pipeline

pk_llm = pipeline(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    device=0  # use GPU
)

print(pk_llm("who is prabhat kumar")[0]['generated_text'])

from datasets import load_dataset
raw_data = load_dataset("json",data_files="/content/prabhat_dat.json")
raw_data

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-0.5B-Instruct"
)

# sample = raw_data["train"][10]

def preprocess(sample):
    sample = sample["prompt"] + "\n" + sample["completion"]

    ## tokenizer will covert each word into integer as index
    tokenized = tokenizer(
        sample,
        max_length = 128, # length max
        truncation=True,  # cut the length if exceed 128
        padding="max_length" # if sample less than 128 then add 0 and 1
    )

    tokenized["labels"] = tokenized["input_ids"].copy() # label is necessary for the training

    return tokenized
data = raw_data.map(preprocess)

print(data["train"][8])

"""## As in above input we can see the with input_ids,attention_mask and labels we also got our real prompt so now we will
LoRA .
LoRA stands for Low-Rank Adaptation of Large Language Models.

It is a parameter-efficient fine-tuning technique used to adapt large pre-trained models (like LLMs) to new tasks without updating all the model parameters. Instead, it introduces and trains a small number of new parameters, significantly reducing memory and compute requirements.

## LoRA
"""

# Let's use ,LoRA
# from peft import LoraConfig, get_peft_model, TaskType
# from transformers import AutoModelForCausalLM
# import torch

# model = AutoModelForCausalLM.from_pretrained(
#     "Qwen/Qwen2.5-3B-Instruct",
#     device_map="cuda",
#     torch_dtype=torch.float16
# )

# lora_config = LoraConfig(
#     task_type=TaskType.CAUSAL_LM,
#     target_modules=["q_proj", "k_proj", "v_proj"]
# )

# model = get_peft_model(model, lora_config)

from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-0.5B-Instruct",
    device_map="cuda",  # Or "cuda" if single GPU
    torch_dtype=torch.float16  # Use fp16 for speed on GPU
)

# Adjust LoRA config (same target modules as before)
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj"]
)
model = get_peft_model(model, lora_config)

# ## Training
# from transformers import TrainingArguments, Trainer

# train_args = TrainingArguments(
#     num_train_epochs=7,
#     learning_rate = 0.001,
#     logging_steps = 25,
#     fp16=True
# )

# trainer = Trainer(
#     args=train_args,
#     model = model,
#     train_dataset=data["train"]
# )
from transformers import TrainingArguments, Trainer

train_args = TrainingArguments(
    output_dir="./trainer_output",           # Required
    num_train_epochs=12,
    learning_rate=0.001,
    logging_steps=25,
    fp16=True
    # Removed: fp16, evaluation_strategy, save_strategy, report_to
)

trainer = Trainer(
    args=train_args,
    model=model,
    train_dataset=data["train"]
)

trainer.train()

trainer.save_model("./prabhat_llma")
tokenizer.save_pretrained("./prabhat_llma")

ask_llm = pipeline(
    model="./prabhat_llma",
    tokenizer="./prabhat_llma",
    device="cuda",
    torch_dtype=torch.float16
)

ask_llm("Who is Prabhat Kumar?")[0]['generated_text']

